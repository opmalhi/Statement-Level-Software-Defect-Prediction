{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in c:\\users\\om\\anaconda3\\lib\\site-packages (68.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\om\\anaconda3\\lib\\site-packages (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "     ---------------------------------------- 11.6/11.6 MB 6.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\om\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\om\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\om\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\om\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 tzdata-2024.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\om\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\om\\anaconda3\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\om\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\om\\anaconda3\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.12)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.54.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.23.3)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\om\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.12.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\om\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\om\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\om\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\om\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\om\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\om\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\om\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\om\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\om\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\om\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install setuptools\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype \n",
    "from tensorflow import keras \n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.activations import *\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "import itertools\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from itertools import product\n",
    "import math\n",
    "from utils import lex\n",
    "from utils import yacc\n",
    "from utils import cpp\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here We Will Configure The Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_name = \"./newdata.zip\"\n",
    "\n",
    "# The collumns containing the code info\n",
    "cols = [\"code\", \"block\"]\n",
    "# The Archive Containing The Actual Codes\n",
    "archive = zipfile.ZipFile('./newdata.zip', 'r')\n",
    "# The first name is the name of the containing folder of the codes \n",
    "list_files = archive.namelist()[1:]\n",
    "# Now We will make a scanner for the C++ language\n",
    "scanner = lex.lex(cpp)\n",
    "# If any thing was considered fault at the line i, we will consider all the lines [i - range_n, i + range_n) to be fault \n",
    "range_n = 2\n",
    "# Then We Define The literals of the program\n",
    "lits = cpp.literals\n",
    "# Then We Define The Tokens\n",
    "toks = list(cpp.tokens)\n",
    "# We remove the White Space token to add it later \n",
    "toks.remove(\"CPP_WS\")\n",
    "# We add the White Space token here because we want it to have the value of zero, we'll use this latter for padding lines of code\n",
    "toks.insert(0, \"CPP_WS\")\n",
    "# Tok 2 N : a dictionary from tokens to thier integer, mapped, value\n",
    "tok2n = dict(zip(toks + [i for i in lits], itertools.count()))\n",
    "# N 2 Tok : a dictionary from integers to thier token, mapped, value\n",
    "n2tok = dict(zip(itertools.count(), toks + [i for i in lits]))\n",
    "\n",
    "# The maximum value we allow in as a constant value in a code\n",
    "max_v = 2147483647 - 1\n",
    "\n",
    "# The amount of importance we give to 1s 0s and false postives and false negatives\n",
    "WEIGHTS_FOR_LOSS = np.array([[2,0.5],[0.1,0.1], [0.1,0.1], [0.1,0.1], [0.1,0.1]]) #TODO: add same false positive and negative as error line to other classes \n",
    "#added temporary weights for classes 2,3,4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here We Will Make A Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_function(weights, rnn=True):\n",
    "        \n",
    "    '''\n",
    "    gives us the loss function\n",
    "    '''\n",
    "    def w_categorical_crossentropy_mine(y_true, y_pred):\n",
    "        nb_cl = len(weights)\n",
    "        \n",
    "        if(not rnn):\n",
    "            final_mask = K.zeros_like(y_pred[:, 0])\n",
    "            y_pred_max = K.max(y_pred, axis=1)\n",
    "            y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], 1))\n",
    "            y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "            for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "                final_mask += ( weights[c_t, c_p] * K.cast(y_pred_max_mat, tf.float32)[:, c_p] * K.cast(y_true, tf.float32)[:, c_t]  )\n",
    "            return K.categorical_crossentropy(y_true, y_pred, True) * final_mask \n",
    "        else:\n",
    "            final_mask = K.zeros_like(y_pred[:, :,0])\n",
    "            y_pred_max = K.max(y_pred, axis=2)\n",
    "            y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], K.shape(y_pred)[1], 1))\n",
    "            y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "            for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "                final_mask += ( weights[c_t, c_p] * K.cast(y_pred_max_mat, tf.float32)[:, :,c_p] * K.cast(y_true, tf.float32)[:, :,c_t]  )\n",
    "            return K.categorical_crossentropy(y_true, y_pred, True) * final_mask \n",
    "\n",
    "            \n",
    "    return w_categorical_crossentropy_mine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading The Data From The Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def get_data(list_files, archive, log = False):\n",
    "    '''\n",
    "    reads the data and handles the range_n number\n",
    "    '''\n",
    "    res = []\n",
    "    for i in list_files:\n",
    "        try :\n",
    "            x = pd.read_csv(archive.open(i), sep = \"`\")\n",
    "            x = x[x.columns[:-1]]\n",
    "            res.append(x)\n",
    "        except Exception: \n",
    "            print(i)\n",
    "            continue\n",
    "    resF = []\n",
    "    for n_i, i in enumerate(res) :\n",
    "        \n",
    "        if i.shape[0] == 0 :\n",
    "            continue\n",
    "            \n",
    "        a = i.values\n",
    "        b = i.copy()\n",
    "#         This line of code will change the data from (Features, class) to (Features, one-hot-vector)\n",
    "        out_classes = [0, 1, 2, 3, 4]\n",
    "\n",
    "       \n",
    "        cat_type = CategoricalDtype(categories=out_classes, ordered=True)\n",
    "        try:\n",
    "            s_cat = i.iloc[:, -1].astype(int)\n",
    "        except:\n",
    "            print(f'i have shape {i.shape} but i choose to misbehave')\n",
    "            continue\n",
    "       \n",
    "        s_cat = s_cat.astype(cat_type)\n",
    "        one_hot_encodings = pd.get_dummies(s_cat, dtype=int)\n",
    "        # print(f'type: {type(one_hot_encodings)}\\n shape: {one_hot_encodings.shape}')\n",
    "        \n",
    "        b = b.drop(b.columns[-1], axis=1) \n",
    "        # print(f'num cols after label drop: {b.shape}\\n num OHE: {one_hot_encodings.shape} ')\n",
    "        # Concatenate the original DataFrame with the dummy variables DataFrame\n",
    "        b = pd.concat([b, one_hot_encodings], axis=1) #uses pd.get_dummies to one-hot encode instead of exclusive or\n",
    "        b = b.values\n",
    "        # print(f'encoding shape: {b.shape}')\n",
    "        # b = np.concatenate([b[:, :-1], b[:, -1:].astype(int) ^ 1, b[:, -1:]], axis = -1) #TODO: change this so the last cols represent all classes\n",
    "        \n",
    "        \n",
    "        for j in range(len(b)):\n",
    "            if np.sum(a[j - range_n : j + range_n, -1]) > 0 :\n",
    "#                 This was explained before the declaration of range_n\n",
    "#               TODO: decide strategy to represent all lines around errors if multiple errors are encountered in close proximity \n",
    "#               TODO: maybe give first preference to the first occuring error\n",
    "                # b[j, -1] = 1\n",
    "                # b[j, -2] = 0\n",
    "\n",
    "                #steps:\n",
    "                #1. find first non-zero element\n",
    "                # Find the first non-zero index within the range\n",
    "                # first_nonzero = a.iloc[j - range_n : j + range_n, -1][a.iloc[j - range_n : j + range_n, -1] != 0].idxmin()\n",
    "                first_nonzero = next((num for num in a[j - range_n : j + range_n, -1] if num != 0), None)\n",
    "\n",
    "                #assign label based on non-zero element encountered\n",
    "                if first_nonzero == 1:\n",
    "                    b[j, -1] = 0\n",
    "                    b[j, -2] = 0\n",
    "                    b[j, -3] = 0\n",
    "                    b[j, -4] = 1\n",
    "                    b[j, -5] = 0\n",
    "                elif first_nonzero == 2:\n",
    "                    b[j, -1] = 0\n",
    "                    b[j, -2] = 0\n",
    "                    b[j, -3] = 1\n",
    "                    b[j, -4] = 0\n",
    "                    b[j, -5] = 0\n",
    "                elif first_nonzero == 3:\n",
    "                    b[j, -1] = 0\n",
    "                    b[j, -2] = 1\n",
    "                    b[j, -3] = 0\n",
    "                    b[j, -4] = 0\n",
    "                    b[j, -5] = 0\n",
    "                else:              #if first_nonzero_index == 4:\n",
    "                    b[j, -1] = 1\n",
    "                    b[j, -2] = 0\n",
    "                    b[j, -3] = 0\n",
    "                    b[j, -4] = 0\n",
    "                    b[j, -5] = 0\n",
    "                \n",
    "\n",
    "\n",
    "        for x in range(len(b)):\n",
    "            for y in range(len(b[x])):\n",
    "#                 Here we will try to change any thing that is not the code it self and which is a string into numbers \n",
    "                if y > 1 :\n",
    "                    if type(b[x, y]) == str :\n",
    "                        try :\n",
    "                            float(b[x, y].strip())\n",
    "                        except Exception : \n",
    "                            b[x, y] = -3\n",
    "                elif y == 1 :\n",
    "                    b[x, y] = \"DATA DOES NOT MATTER\"\n",
    "#         By 0s we mean the code being fine and so on\n",
    "        b = pd.DataFrame(b, columns=list(i.columns)[:-1] + [\"0s\", \"1s\", \"2s\", \"3s\", \"4s\"]) #changed num classes, added 2,3,4\n",
    "        b.replace(\"#empty\", np.nan, inplace =True)\n",
    "        resF.append(b.dropna())\n",
    "        \n",
    "    if log :     \n",
    "        print(\"data was read and changed\")    \n",
    "    return resF\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Scanner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replacement(scanner, string_in):\n",
    "    \n",
    "    '''\n",
    "    gets a string and returns the None, 2 which is the tokenized version\n",
    "    '''\n",
    "    try :\n",
    "        scanner.input(string_in)\n",
    "    except Exception as e :\n",
    "        print(\"Exception in using the lex\", e)\n",
    "        print(string_in)\n",
    "    token = scanner.token()\n",
    "    \n",
    "    \n",
    "#     id2n and n2id are the same as n2tok tok2n but they are extended to contain the information of the symbol table of each code separately\n",
    "    id2n = dict(zip([i for i in lits], [tok2n[i] for i in lits]))\n",
    "    n2id = dict(zip([tok2n[i] for i in lits], [i for i in lits]))\n",
    "    \n",
    "    n_id = len(lits) + 1\n",
    "    \n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    while token is not None :\n",
    "        \n",
    "        t = token.type\n",
    "        \n",
    "#         If we have recieved a token and it is not something we need to use ord for\n",
    "        if t in cpp.tokens :\n",
    "#             Reciving a white space\n",
    "            if token.type == cpp.tokens[cpp.tokens.index(\"CPP_WS\")]:\n",
    "                #this is because this will make it easier for us to pad our data\n",
    "                v = 0\n",
    "#             Reciving an ID from the code\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_ID\")]:\n",
    "                v = token.value\n",
    "#                 Checking if need to add the id to n2id or not\n",
    "                if v in id2n.keys() :\n",
    "                    pass\n",
    "                else :\n",
    "                    id2n[v] = n_id\n",
    "                    n2id[n_id] = v\n",
    "                    \n",
    "                    n_id += 1\n",
    "                v = id2n[v]\n",
    "#             If we receive a string (We don't use the value of strings)\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_STRING\")]:\n",
    "                v = -1\n",
    "#             If we recive #\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_POUND\")]:\n",
    "                v = -2\n",
    "#             If we recive ##\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_DPOUND\")]:\n",
    "                v = -3\n",
    "#             If we recive char\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_CHAR\")]:\n",
    "                v = -4\n",
    "            elif token.type in cpp.tokens[3:]:\n",
    "                print(\"some thing went really wrong\")\n",
    "#             Parsing the value of constant values\n",
    "            else:\n",
    "                try :\n",
    "                    tv = token.value.lower()\n",
    "                    if tv[-1] == \"l\" : \n",
    "                        tv = tv[:-1]\n",
    "                    if tv[-1] == \"u\" : \n",
    "                        tv = tv[:-1]\n",
    "                    if \"x\" in  tv :\n",
    "                        v = int(tv, base = 16)\n",
    "                    elif tv[-1].lower() == \"l\":\n",
    "                        if tv[-2].lower() == \"u\" :\n",
    "                            v = float(tv[:-2])\n",
    "                        else :\n",
    "                            v = float(tv[:-1])\n",
    "                    else :\n",
    "                        v = float(tv)\n",
    "                    v = np.clip(v, - max_v, max_v)\n",
    "                    \n",
    "                except Exception as e :\n",
    "                    print(\"Couldn't scan this number\", token)\n",
    "                    return\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "        else :\n",
    "            v = ord(t)\n",
    "        try :\n",
    "            t = tok2n[t]\n",
    "        except Exception :\n",
    "            n = len(id2n.keys()) + 1 \n",
    "            tok2n[t] = n\n",
    "            n2tok[n] = t\n",
    "            id2n[t] = n\n",
    "            n2id[n] = t\n",
    "            t = tok2n[t]\n",
    "            \n",
    "        res.append([t, v])\n",
    "        token = scanner.token()\n",
    "        \n",
    "    res = np.array(res)\n",
    "    \n",
    "    return res\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here We Will Tokenize Our Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    \n",
    "    '''\n",
    "    reads data and tokenizes each of the sentences and adds them together.\n",
    "    The out put will contain the actual data, max number of lines per code and mean number of lines per code\n",
    "    the actual data will have the following shape :\n",
    "    \n",
    "    Number of codes, Number of lines per each code , 2 (Data and State)\n",
    "    State will contain (Code being right, Code Being Wrong)\n",
    "    Data Will Contain (Number Of Words, 2 (Token, Value))\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    res = []\n",
    "    x = []\n",
    "    mean = 0\n",
    "    max_num = 0\n",
    "    for i in data:\n",
    "#         If We had any code submissions that was empty, we skip them\n",
    "        if i.shape[0] == 0 :\n",
    "            continue \n",
    "        temp = []\n",
    "        mean += i.shape[0]\n",
    "        max_num = max(max_num, i.shape[0])\n",
    "        \n",
    "        for j in i.values :\n",
    "            \n",
    "            try :\n",
    "                tok = get_replacement(scanner, j[0]).astype(np.float32)\n",
    "            except Exception as e :\n",
    "                continue\n",
    "                \n",
    "            x.append(tok)\n",
    "            \n",
    "            #TODO change j[-2:] so it gets all available classes \n",
    "            y = j[-5:] #changed range to 5 classes instead of 2\n",
    "            temp.append([tok, y])\n",
    "            \n",
    "        res.append(temp)\n",
    "    mean /= len(res)\n",
    "    \n",
    "    return res, mean, max_num\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding The Data In The Statement Level (Adding Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_cols(num, res, empty):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    pads or removes data so they all have the same shape in one code  file \n",
    "    \n",
    "    num : amount of word we'll have per each line\n",
    "    empty : what we'll use to pad our data with\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    resF = []\n",
    "    temp_i = 0\n",
    "    for i in res :\n",
    "        \n",
    "        temp = []\n",
    "        \n",
    "        if (len(i) == 0):\n",
    "#           We'll any coding file which is empty\n",
    "            continue \n",
    "            \n",
    "        for j in i :\n",
    "            \n",
    "#             J[0] is the data and J[1] is the state\n",
    "\n",
    "            if len(j[0]) < num :\n",
    "                \n",
    "                result = np.concatenate([j[0], np.ones(( num - len(j[0]), 2)) * empty], axis = 0)\n",
    "                \n",
    "            elif len(j[0]) > num :\n",
    "                result = j[0][:num, :]\n",
    "            else :\n",
    "                result = j[0]\n",
    "                \n",
    "            result = result.reshape((-1))\n",
    "            \n",
    "#             This is so that we'll have the data and our state at the same time\n",
    "            result = np.concatenate([result, np.array([j[1]]).reshape((-1))], axis = 0)\n",
    "            temp.append(np.array(result))\n",
    "\n",
    "        # print(f'temp shape at {temp_i}: {len(temp)}')\n",
    "        # temp_i += 1\n",
    "        resF.append(np.array(temp))\n",
    "        \n",
    "       \n",
    "    resF = np.asarray(resF, dtype=\"object\")\n",
    "    \n",
    "    \n",
    "    return resF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating The 32 Processed Columns And Lexical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_data(tokenized_final, data):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    adds the information from the parser to the things that were gained from the information of scanners\n",
    "    tokenized_final will be the output of \"change_cols\" and data will be the output of \"get_data\"\n",
    "    '''\n",
    "#     The first line reads data and drops the following columns : columns containing text of the parser or lex and the \n",
    "#     The last two columns which are the state of the code which we are trying to predict\n",
    "    dataR = np.concatenate([i.drop(cols, axis = 1).values[:, :-5] for i in data], axis = 0) #TODO change -2 so it matches len of available labels\n",
    "    dataR = dataR.astype(np.float32)\n",
    "    \n",
    "    cnt = 0 \n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    \n",
    "    for i in tokenized_final : \n",
    "        temp = []\n",
    "        for j in i :\n",
    "            \n",
    "            add = dataR[cnt, :]\n",
    "            temp.append(np.concatenate([add, j], axis = 0))\n",
    "            \n",
    "            cnt += 1\n",
    "            \n",
    "            \n",
    "        res.append(np.array(temp))\n",
    "    res = np.asarray(res, dtype=\"object\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using All The Functions Above And Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(list_data, archive, scaler = None, add_all = False, type_add = 0,\n",
    "                pad1 = None, pad2 = None, return_before_pad = False, cons_per_line = 10, log  = False):\n",
    "    '''\n",
    "    Reading, Tokenizing, Concatenating And Normalizing Data\n",
    "    list_data : Name Of The Codes We Are Using\n",
    "    scalar : Scalar Used To Normalize Data, If None Is Presented, The Function Will Compute One\n",
    "    add_all : Whether Or Not We Want All Our Codes To Have The Same Amount As For The Lines Of Code\n",
    "    type_add : The amount of lines each code should contain : {0 : mean, 1 : max number of lines}\n",
    "    pad1 : The amount of words each line should contain, If None is presented const_per_line + mean(amount of words per line) would be used\n",
    "    pad2 : The amount of lines each code should contain, type_add would not be used if pad2 is not None\n",
    "    return_before_pad : Whether or not to also return the data before it was padded to have the same amount of lines percode \n",
    "    '''\n",
    "#     First We Read Our Data From The Zip File\n",
    "    data = get_data(list_data, archive)\n",
    "#     The We tokenize our data\n",
    "    r, mean, max_num = tokenize_data(data)\n",
    "    \n",
    "#     Then We Create Our Empty Vector\n",
    "    empty = np.array([tok2n[\"CPP_WS\"], 0]).reshape(1, 2).astype(np.float32)\n",
    "    \n",
    "#     The Defualt Option for Padding \n",
    "    if pad1 is None :\n",
    "        pad1 = int(mean) + cons_per_line\n",
    "        \n",
    "#     We Padd Our Data At Each Line With Extra White Spaces\n",
    "    res = change_cols(pad1, r, empty)\n",
    "    r = np.array(res)\n",
    "    \n",
    "#     Here we will concatenate our lexical features and our preprocessed features\n",
    "    r = get_final_data(r, data)\n",
    "    if log :\n",
    "        print(\"Padded The Lexical And Preprocessed Features of Data\")\n",
    "    \n",
    "    \n",
    "    res = np.asarray(r).astype('object')\n",
    "    \n",
    "    if add_all :\n",
    "        \n",
    "        if log :\n",
    "            print(\"Computing How Many Empty Line To Add To Codes \")\n",
    "        if pad2 is None :\n",
    "            \n",
    "            mean = 0\n",
    "            max_num = -1\n",
    "            for i in r :\n",
    "                mean += i.shape[0]\n",
    "                max_num = max(max_num, i.shape[0])\n",
    "                \n",
    "                \n",
    "            mean /= r.shape[0]\n",
    "            nums = [int(mean), max_num]\n",
    "            pad2 = nums[type_add]\n",
    "            \n",
    "        res = []\n",
    "        if log :\n",
    "            print(\"Computed How Many Empty Line To Add To Codes \")\n",
    "        for i in r :\n",
    "            \n",
    "            if i.shape[0] < pad2 :\n",
    "                \n",
    "                zeros = np.zeros([pad2 - i.shape[0], i.shape[1]])\n",
    "                zeros[:, -2] = 1 #TODO change -2 so it matches len of available labels #changed to -5\n",
    "                temp = np.concatenate([i, zeros], axis = 0)\n",
    "            elif i.shape[0] > pad2 :\n",
    "                temp = i[:pad2, :]\n",
    "            else :\n",
    "                temp = i\n",
    "            res.append(temp)\n",
    "        if log :\n",
    "            print(\"Added All The Empty Lines \")\n",
    "\n",
    "    res = np.array(res)\n",
    "    \n",
    "    save_r = r.copy()\n",
    "    \n",
    "    r = np.concatenate(res, axis = 0).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    if scaler is None :\n",
    "        \n",
    "        scaler = StandardScaler().fit(r[:, :-5].astype(np.float32)) #TODO change -2 so it matches len of available labels #changed to -5\n",
    "        if log :\n",
    "            print(\"Computed Mean And Standard Deviation For Normalization \")\n",
    "    \n",
    "    for i, iv in enumerate(res) :\n",
    "        \n",
    "        res[i, :, :-5] = scaler.transform(iv[:, :-5].astype(np.float32)).astype(np.float32) #TODO change -2 so it matches len of available labels #changed to -5\n",
    "        \n",
    "        if log :\n",
    "            print(\"Data Was Normalized \")\n",
    "\n",
    "    \n",
    "    if return_before_pad :\n",
    "        return res, scaler, pad1, pad2, save_r\n",
    "        \n",
    "    return res, scaler, pad1, pad2\n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "                 \n",
    "                \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making The Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(shape):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    gets the first rnn model\n",
    "    shape : shape of the input : shape of the codes [Number of lines (which can be None), Number of Fetures per line]\n",
    "    '''\n",
    "    in1 = Input(shape)\n",
    "    X = Bidirectional(LSTM(150, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(in1)\n",
    "    X = LSTM(150, return_sequences=True, dropout=0.25, recurrent_dropout=0.1,)(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(256, activation=relu)(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = Dense(128, activation=relu)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Dense(64, activation=relu)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = Dense(32, activation=relu)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    X = Dense(16, activation=relu)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    \n",
    "    X = Dense(5, activation=softmax)(X) #TODO change 2 so it matches len of available labels # changed out layer to 5\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(in1, X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Measurements For Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(y_true, y_pred):\n",
    "    \n",
    "    # acc = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3, 4]) #TODO change labels to match the num of classes #added 2,3,4\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    rec1 = recall_score(y_true, y_pred, average='weighted')\n",
    "    prec1 = precision_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    #replacement formulae\n",
    "    #TP = sum(acc[i][i])\n",
    "    #FN = sum(acc[i]) - acc[i][i]. ie sum across rows minus TP\n",
    "    #FP = sum(acc[][i]) - acc[i][i]. ie. sum across column - TP\n",
    "    #TN = sum(acc) - (sum(acc[i]) + sum(acc[][i])) \n",
    "\n",
    "    #calculate per class, then use weighted avg since specific error classified as other column shouldn't be as bad \n",
    "#     True Positive\n",
    "#     tp =  acc[1][1]\n",
    "# #     False Negative\n",
    "#     fn =  acc[1][0]\n",
    "# #     False Positive\n",
    "#     fp =  acc[0][1]\n",
    "# #     True Negative\n",
    "#     tn =  acc[0][0]\n",
    "# #     Recall\n",
    "#     rec1 = acc[1][1] / (acc[1][1] + acc[1][0])\n",
    "# #     Precision \n",
    "#     prec1 = acc[1][1] / (acc[1][1] + acc[0][1])\n",
    "# #     Over ALl Accuracy\n",
    "#     accuracy = (acc[1][1] + acc[0][0]) / (acc[1][1] + acc[0][0] + acc[1][0] + acc[0][1])\n",
    "# #     F1 Accuracy\n",
    "#     f1 = 2.0 / ((1.0/rec1) + (1.0/prec1))\n",
    "    \n",
    "    return rec1, prec1, acc, f1 #, tp , fn , fp , tn,  accuracy changed to acc\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Paring Data For Measurements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mus(y_true, x, model):\n",
    "    \n",
    "    y_t = np.argmax(y_true, axis = -1).reshape((-1))\n",
    "    y_p = model.predict(x)\n",
    "    y_p = np.argmax(y_p, axis=-1).reshape((-1))\n",
    "    value_counts = pd.Series(y_p).value_counts()\n",
    "    \n",
    "    \n",
    "    return get_acc(y_t, y_p), value_counts\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using All The Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 0\n",
      "newdata/validated/\n",
      "data read\n",
      "training on the data started \n",
      "training on the data finished \n",
      "train : rec1, prec1, accuracy, f1\n",
      "3375/3375 [==============================] - 142s 41ms/step\n",
      "(0.9709892978410969, 0.9707119079431863, 0.9709892978410969, 0.9707447875027965)\n",
      "Train: error type counts for fold 0:\n",
      "0    1095776\n",
      "3     485371\n",
      "1     306425\n",
      "2     128857\n",
      "4      35400\n",
      "Name: count, dtype: int64\n",
      "test : rec1, prec1, accuracy, f1\n",
      "375/375 [==============================] - 16s 42ms/step\n",
      "(0.9484563216676464, 0.9478569800698529, 0.9484563216676464, 0.9478496898870431)\n",
      "Test: error type counts for fold 0:\n",
      "0    125710\n",
      "3     43903\n",
      "1     36700\n",
      "2     16306\n",
      "4      5343\n",
      "Name: count, dtype: int64\n",
      "k 1\n",
      "newdata/validated/\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 775. MiB for an array with shape (2051810, 99) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# gathering data for train\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m r_train, scaler, pad1, pad2 \u001b[38;5;241m=\u001b[39m \u001b[43mgather_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_all\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# gathering data for test, please note that the same mean and standard deviation that was computed for train will be used to \u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# normalize test data and also the information of pad1 and pad2 is computed from train so that no information will be \u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# leaked from train and also none of the aforementioned are dependent on the test data \u001b[39;00m\n\u001b[0;32m     34\u001b[0m r_test, _, _, _ \u001b[38;5;241m=\u001b[39m gather_data(data_test, archive, scaler \u001b[38;5;241m=\u001b[39m scaler, add_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, pad1 \u001b[38;5;241m=\u001b[39m pad1, pad2 \u001b[38;5;241m=\u001b[39m pad2)\n",
      "Cell \u001b[1;32mIn[11], line 76\u001b[0m, in \u001b[0;36mgather_data\u001b[1;34m(list_data, archive, scaler, add_all, type_add, pad1, pad2, return_before_pad, cons_per_line, log)\u001b[0m\n\u001b[0;32m     72\u001b[0m res \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(res)\n\u001b[0;32m     74\u001b[0m save_r \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 76\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m :\n\u001b[0;32m     81\u001b[0m     scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\u001b[38;5;241m.\u001b[39mfit(r[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)) \u001b[38;5;66;03m#TODO change -2 so it matches len of available labels #changed to -5\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 775. MiB for an array with shape (2051810, 99) and data type float32"
     ]
    }
   ],
   "source": [
    "# K is for our K-fold\n",
    "k = 10\n",
    "# The name of the codes we want to use, you can slice this list to a smaller list for a fast test\n",
    "l = list_files[:]\n",
    "# The number of codes we use on each fold\n",
    "size = math.ceil(len(l) / k)\n",
    "# Verbose for our NN model \n",
    "verbose = 0 \n",
    "\n",
    "# The results for trains and tests respectivly\n",
    "trs = []\n",
    "ts = []\n",
    "\n",
    "for i in range(k):\n",
    "    \n",
    "    print(\"k\", i)\n",
    "#     start and end will be the indicies for what we'll use for test\n",
    "    start  = i * size\n",
    "    end    = min(len(l), (i + 1) * size)\n",
    "    \n",
    "    data_train = l[:start] + l[end:]\n",
    "    data_test  = l[start : end]\n",
    "    \n",
    "    if len(data_test) <= 0 or len(data_train) <= 0 :\n",
    "        print(\"Insufficient data\")\n",
    "        continue\n",
    "\n",
    "   \n",
    "    # gathering data for train\n",
    "    r_train, scaler, pad1, pad2 = gather_data(data_train, archive, add_all = True)\n",
    "    # gathering data for test, please note that the same mean and standard deviation that was computed for train will be used to \n",
    "    # normalize test data and also the information of pad1 and pad2 is computed from train so that no information will be \n",
    "    # leaked from train and also none of the aforementioned are dependent on the test data \n",
    "    r_test, _, _, _ = gather_data(data_test, archive, scaler = scaler, add_all = True, pad1 = pad1, pad2 = pad2)\n",
    "    \n",
    "    print(\"data read\")\n",
    "    \n",
    "    # configuring model\n",
    "    model = get_model([None, r_train.shape[-1] - 5])\n",
    "    loss = get_loss_function(WEIGHTS_FOR_LOSS)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate = 1e-3), keras.losses.categorical_crossentropy, metrics = [\"accuracy\"])\n",
    "\n",
    "    \n",
    "    # making the X, y for train and test set\n",
    "    X_train = r_train[:, :, :-5] #changed -2 to -6 since there are 5 classes\n",
    "    y_train = r_train[:, :, -5:] #changed -2 to -6 since there are 5 classes\n",
    "    \n",
    "    # temp_line = '--' * 50\n",
    "    # print(f'Variable shapes\\n{temp_line}\\nX_train: {X_train.shape}\\n\\ny_train: {y_train.shape} ')\n",
    "\n",
    "    X_test = r_test[:, :, :-5] #changed -2 to -6 since there are 5 classes\n",
    "    y_test = r_test[:, :, -5:] #changed -2 to -6 since there are 5 classes\n",
    "\n",
    "    # print(f'Variable shapes\\n{temp_line}\\nX_test: {X_test.shape}\\n\\ny_test: {y_test.shape} ')\n",
    "\n",
    "    X_train = tf.convert_to_tensor(X_train, tf.float32)\n",
    "    X_test = tf.convert_to_tensor(X_test, tf.float32)\n",
    "\n",
    "    # y_train = np.array(y_train).astype(np.int_)\n",
    "    # y_test = np.array(y_test).astype(np.int_)\n",
    "\n",
    "    y_train = tf.convert_to_tensor(y_train, tf.int32)  \n",
    "    y_test = tf.convert_to_tensor(y_test, tf.int32)\n",
    "\n",
    "    \n",
    "\n",
    "    # print(f'Params and their types \\n X_train: {type(X_train[0])}\\ny_train: {type(y_train[0][0])}\\nX_test: {type(X_test[0])}\\ny_test: {type(y_test[0][0])} ')\n",
    "    \n",
    "    # training the model\n",
    "    print(\"training on the data started \")\n",
    "    model.fit(X_train, y_train, validation_data = [X_test, y_test], epochs = 20, batch_size = 8, verbose = verbose)\n",
    "    print(\"training on the data finished \")\n",
    "    \n",
    "    # saving and printing the accuracy of training data\n",
    "    print(\"train : rec1, prec1, accuracy, f1\")\n",
    "    train_results, train_value_counts = get_mus(y_train, X_train,model)\n",
    "    trs.append(train_results)\n",
    "    print(trs[-1])\n",
    "\n",
    "    print(f'Train: error type counts for fold {i}:')\n",
    "    print(train_value_counts)\n",
    "\n",
    "\n",
    "\n",
    "    # preparing to write the training accuracy to a file\n",
    "    strRes = \"train : \"\n",
    "    for counter in range(len(trs[-1])):\n",
    "        strRes = strRes + '%.5f' % trs[-1][counter] + \" , \"\n",
    "    \n",
    "    strRes += \" \\n \"\n",
    "    \n",
    "\n",
    "    # saving and printing the accuracy of testing data\n",
    "    print(\"test : rec1, prec1, accuracy, f1\")\n",
    "    test_results, test_value_counts = get_mus(y_test, X_test,model)\n",
    "    ts.append(test_results)\n",
    "    print(ts[-1])\n",
    "\n",
    "    print(f'Test: error type counts for fold {i}:')\n",
    "    print(test_value_counts)\n",
    "    \n",
    "    # preparing to write the testing accuracy to a file\n",
    "    strRes += \" test :  \"\n",
    "    for counter in range(len(ts[-1])): #changed 8 to len(ts[-1]) to get real tuple len\n",
    "        strRes = strRes + '%.5f' % ts[-1][counter] + \" , \"\n",
    "    \n",
    "    # writing the accuracies on to a file\n",
    "    f = open(\"test.txt\", \"a\")\n",
    "    f.write(strRes + \"\\n\")\n",
    "    f.close()\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg train : rec1, prec1, accuracy, f1\n",
      "[0.95714724 0.95792131 0.95714724 0.95728996]\n",
      "avg test : rec1, prec1, accuracy, f1\n",
      "[0.95228773 0.95298652 0.95228773 0.95238463]\n"
     ]
    }
   ],
   "source": [
    "trs = np.array(trs)\n",
    "ts = np.array(ts)\n",
    "print(\"avg train : rec1, prec1, accuracy, f1\")\n",
    "print(np.mean(trs[:, : 4], axis=0))\n",
    "print(\"avg test : rec1, prec1, accuracy, f1\")\n",
    "print(np.mean(ts[:, : 4], axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
